{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from utils.Param import FTFYParam\n",
    "from network.dataset.ftfy_patchdata import input_fn\n",
    "from network.model_fn import ftfy_model_fn\n",
    "from network.train import FTFYEstimator\n",
    "\n",
    "from utils.eval import calc_iou_k, ftfy_retrieval_accuracy\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproduction\n",
    "np.random.seed(2019)\n",
    "tf.set_random_seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters (adjust as needed)\n",
    "param = FTFYParam(ftfy_scope='ftfy', feat_scope='triplet-net', log_dir=None)\n",
    "param.is_ftfy_model = True\n",
    "param.batch_size = 8 # 32 for v100\n",
    "feat_trainable = False\n",
    "\n",
    "log_dir = './log/sem_ftfy_full'\n",
    "param.model_path = './log/sem_ftfy_full/ckpt' # './log/sem'\n",
    "param.train_datasets = 'sem' \n",
    "param.test_datasets = 'human'\n",
    "\n",
    "is_sem = param.test_datasets == 'sem'\n",
    "if is_sem:\n",
    "    param.data_dir = '/home/sungsooha/Desktop/Data/ftfy/sem/train'\n",
    "else:\n",
    "    param.data_dir = '/home/sungsooha/Desktop/Data/ftfy/austin'\n",
    "    #param.train_datasets = 'campus'\n",
    "    param.src_dir = 'human_sources'\n",
    "    param.tar_dir = 'human_patch'\n",
    "    param.tar_patches_per_row = 13\n",
    "    param.tar_patches_per_col = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_sem:\n",
    "    # only for sem dataset\n",
    "    data_dirs = []\n",
    "    for data_dir in os.listdir(param.data_dir):\n",
    "        if os.path.isdir(os.path.join(param.data_dir, data_dir)):\n",
    "            data_dirs.append(data_dir)\n",
    "    data_dirs = sorted(data_dirs)\n",
    "else:\n",
    "    data_dirs = ['.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.info(\"Preparing data pipeline ...\")\n",
    "with tf.device('/cpu:0'), tf.name_scope('input'):\n",
    "    dataset, data_sampler = input_fn(\n",
    "        param.data_dir,\n",
    "        batch_size=param.batch_size,\n",
    "        cellsz=param.src_cellsz,\n",
    "        n_parameters=param.n_parameters,\n",
    "        src_size=param.src_size,\n",
    "        tar_size=param.tar_size,\n",
    "        n_channels=param.n_channels\n",
    "    )\n",
    "    data_iterator = tf.data.Iterator.from_structure(\n",
    "        dataset.output_types,\n",
    "        dataset.output_shapes\n",
    "    )\n",
    "    dataset_init = data_iterator.make_initializer(dataset)\n",
    "    batch_data = data_iterator.get_next()\n",
    "\n",
    "data_sampler.load_dataset(\n",
    "    data_dirs, param.src_dir, param.tar_dir,\n",
    "    src_ext=param.src_ext, src_size=param.src_size, n_src_channels=param.n_channels,\n",
    "    src_per_col=param.src_patches_per_col, src_per_row=param.src_patches_per_row,\n",
    "    tar_ext=param.tar_ext, tar_size=param.tar_size, n_tar_channels=param.n_channels,\n",
    "    tar_per_col=param.tar_patches_per_col, tar_per_row=param.tar_patches_per_row\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.info('Loading training stats: %s' % param.train_datasets)\n",
    "file = open(os.path.join(log_dir, 'stats_%s.pkl' % param.train_datasets), 'rb')\n",
    "mean, std = pickle.load(file)\n",
    "tf.logging.info('Mean: {:.5f}'.format(mean))\n",
    "tf.logging.info('Std : {:.5f}'.format(std))\n",
    "data_sampler.normalize_data(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.info(\"Creating the model ...\")\n",
    "sources, targets, labels, bboxes = batch_data\n",
    "spec = ftfy_model_fn(sources, targets, labels, bboxes,\n",
    "                     mode='TEST', **param.get_model_kwargs(feat_trainable=feat_trainable))\n",
    "# 20-th epoch, logged with 5 interval\n",
    "estimator = FTFYEstimator(spec, **param.get_ckpt_kwargs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = [1, 5, 10, 20]\n",
    "iou_thrs = [0.7, 0.8, 0.9]\n",
    "n_iters = 10\n",
    "n_max_tests = 5000\n",
    "\n",
    "avg_accuracy = np.zeros((len(iou_thrs), len(top_k)), dtype=np.float32)\n",
    "avg_d_mean = np.zeros(4, dtype=np.float32)\n",
    "avg_d_std = np.zeros(4, dtype=np.float32)\n",
    "\n",
    "for it in range(n_iters):\n",
    "    tf.logging.info('Try {:d}'.format(it))\n",
    "    \n",
    "    data_sampler.reset()\n",
    "    pred_confidences, pred_bboxes, bboxes = \\\n",
    "        estimator.run(dataset_init,top_k=top_k[-1], n_max_test=n_max_tests)\n",
    "    iou_k = calc_iou_k(pred_bboxes, bboxes)\n",
    "    accuracy = ftfy_retrieval_accuracy(iou_k, top_k, iou_thrs)\n",
    "    \n",
    "    pred_bboxes = pred_bboxes[:, 0]\n",
    "    d_bboxes = np.abs(pred_bboxes - bboxes)\n",
    "    src_h, src_w = param.src_size\n",
    "    d_bboxes[..., 0] *= src_w\n",
    "    d_bboxes[..., 1] *= src_h\n",
    "    d_bboxes[..., 2] *= src_w\n",
    "    d_bboxes[..., 3] *= src_h\n",
    "    d_cx_mean, d_cx_std = np.mean(d_bboxes[..., 0]), np.std(d_bboxes[..., 0])\n",
    "    d_cy_mean, d_cy_std = np.mean(d_bboxes[..., 1]), np.std(d_bboxes[..., 1])\n",
    "    d_w_mean, d_w_std = np.mean(d_bboxes[..., 2]), np.std(d_bboxes[..., 2])\n",
    "    d_h_mean, d_h_std = np.mean(d_bboxes[..., 3]), np.std(d_bboxes[..., 3])\n",
    "    \n",
    "    tf.logging.info('Accuracy:')\n",
    "    tf.logging.info('{}'.format(accuracy))\n",
    "\n",
    "    tf.logging.info('For the best (@k=1), [mean, std]')\n",
    "    tf.logging.info('d_cx: {:.3f}, {:.3f}'.format(d_cx_mean, d_cx_std))\n",
    "    tf.logging.info('d_cy: {:.3f}, {:.3f}'.format(d_cy_mean, d_cy_std))\n",
    "    tf.logging.info('d_w : {:.3f}, {:.3f}'.format(d_w_mean, d_w_std))\n",
    "    tf.logging.info('d_h : {:.3f}, {:.3f}'.format(d_h_mean, d_h_std))\n",
    "        \n",
    "    avg_accuracy += accuracy\n",
    "    avg_d_mean += np.array([d_cx_mean, d_cy_mean, d_w_mean, d_h_mean])\n",
    "    avg_d_std += np.array([d_cx_std, d_cy_std, d_w_std, d_h_std])\n",
    "\n",
    "avg_accuracy /= n_iters\n",
    "avg_d_mean /= n_iters\n",
    "avg_d_std /= n_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracy = np.asarray(avg_accuracy)\n",
    "all_accuracy = np.squeeze(avg_accuracy)\n",
    "plt.plot(avg_accuracy.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.info('For the best (@k=1), [cx, cy, w, h]')\n",
    "tf.logging.info('mean: {}'.format(avg_d_mean))\n",
    "tf.logging.info('std : {}'.format(avg_d_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "out_dir = os.path.join(log_dir, 'eval_metrics_{}_{}.npy'.format(\n",
    "    param.train_datasets, param.test_datasets\n",
    "))\n",
    "metric = dict(\n",
    "    accuracy=avg_accuracy,\n",
    "    d_mean=avg_d_mean,\n",
    "    d_std=avg_d_std\n",
    ")\n",
    "np.save(out_dir, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
