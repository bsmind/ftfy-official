{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from utils.Param import FTFYParam\n",
    "from network.dataset.ftfy_patchdata import input_fn\n",
    "from network.model_fn import ftfy_model_fn\n",
    "from network.train import FTFYEstimator\n",
    "\n",
    "from utils.eval import calc_iou_k, ftfy_retrieval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproduction\n",
    "np.random.seed(2019)\n",
    "tf.set_random_seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters (adjust as needed)\n",
    "param = FTFYParam(ftfy_scope='ftfy', feat_scope='triplet-net', log_dir='./log/sem_ftfy')\n",
    "param.is_ftfy_model = False\n",
    "#param.data_dir = './Data/sem/train'\n",
    "param.model_path = './log/sem/ckpt' # './log/sem'\n",
    "param.batch_size = 4 # 32 for v100\n",
    "feat_trainable = False\n",
    "param.optimizer_name = 'Grad'\n",
    "param.learning_rate = 0.01\n",
    "param.obj_scale = 1.0\n",
    "param.noobj_scale = 0.5\n",
    "param.coord_scale = 5.0\n",
    "param.decay_steps = 100000 # 60000 for v100\n",
    "param.train_log_every = 1000\n",
    "\n",
    "n_max_tests = 1000 # 5000\n",
    "n_max_steps = 1000 # 0 for v100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for sem dataset\n",
    "data_dirs = []\n",
    "for data_dir in os.listdir(param.data_dir):\n",
    "    if os.path.isdir(os.path.join(param.data_dir, data_dir)):\n",
    "        data_dirs.append(data_dir)\n",
    "data_dirs = sorted(data_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.info(\"Preparing data pipeline ...\")\n",
    "with tf.device('/cpu:0'), tf.name_scope('input'):\n",
    "    dataset, data_sampler = input_fn(\n",
    "        param.data_dir,\n",
    "        batch_size=param.batch_size,\n",
    "        cellsz=param.src_cellsz,\n",
    "        n_parameters=param.n_parameters,\n",
    "        src_size=param.src_size,\n",
    "        tar_size=param.tar_size,\n",
    "        n_channels=param.n_channels\n",
    "    )\n",
    "    data_iterator = tf.data.Iterator.from_structure(\n",
    "        dataset.output_types,\n",
    "        dataset.output_shapes\n",
    "    )\n",
    "    dataset_init = data_iterator.make_initializer(dataset)\n",
    "    batch_data = data_iterator.get_next()\n",
    "\n",
    "data_sampler.load_dataset(\n",
    "    data_dirs, param.src_dir, param.tar_dir,\n",
    "    src_ext=param.src_ext, src_size=param.src_size, n_src_channels=param.n_channels,\n",
    "    src_per_col=param.src_patches_per_col, src_per_row=param.src_patches_per_row,\n",
    "    tar_ext=param.tar_ext, tar_size=param.tar_size, n_tar_channels=param.n_channels,\n",
    "    tar_per_col=param.tar_patches_per_col, tar_per_row=param.tar_patches_per_row\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.info('Loading training stats: %s' % param.train_datasets)\n",
    "try:\n",
    "    file = open(os.path.join(param.log_dir, 'stats_%s.pkl' % param.train_datasets), 'rb')\n",
    "    mean, std = pickle.load(file)\n",
    "except FileNotFoundError:\n",
    "    tf.logging.info(\"Calculating train data stats (mean, std)\")\n",
    "    mean, std = data_sampler.generate_stats()\n",
    "    pickle.dump(\n",
    "        [mean, std],\n",
    "        open(os.path.join(param.log_dir, 'stats_%s.pkl' % param.train_datasets), 'wb')\n",
    "    )\n",
    "tf.logging.info('Mean: {:.5f}'.format(mean))\n",
    "tf.logging.info('Std : {:.5f}'.format(std))\n",
    "data_sampler.normalize_data(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.info(\"Creating the model ...\")\n",
    "sources, targets, labels, bboxes = batch_data\n",
    "spec = ftfy_model_fn(sources, targets, labels, bboxes,\n",
    "                     mode='TRAIN', **param.get_model_kwargs(feat_trainable=feat_trainable))\n",
    "# 20-th epoch, logged with 5 interval\n",
    "estimator = FTFYEstimator(spec, **param.get_ckpt_kwargs(20//5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = [1, 5, 10, 20, 30]\n",
    "iou_thrs = [0.7]\n",
    "\n",
    "\n",
    "all_loss = []\n",
    "all_accuracy = []\n",
    "all_d_cx_mean, all_d_cx_std = [], []\n",
    "all_d_cy_mean, all_d_cy_std = [], []\n",
    "all_d_w_mean, all_d_w_std = [], []\n",
    "all_d_h_mean, all_d_h_std = [], []\n",
    "\n",
    "tf.logging.info('='*50)\n",
    "tf.logging.info('Start training ...')\n",
    "tf.logging.info('='*50)\n",
    "for epoch in range(param.n_epoch):\n",
    "    tf.logging.info('-'*50)\n",
    "    tf.logging.info('TRAIN {:d}, {:s} start ...'.format(epoch, param.train_datasets))\n",
    "    loss = estimator.train(\n",
    "        dataset_initializer=dataset_init,\n",
    "        log_every=param.train_log_every,\n",
    "        n_max_steps=n_max_steps\n",
    "    )\n",
    "    all_loss.append(loss)\n",
    "    tf.logging.info('-'*50)\n",
    "\n",
    "    tf.logging.info('-' * 50)\n",
    "    tf.logging.info('TEST {:d}, {:s} start ...'.format(epoch, param.train_datasets))\n",
    "    pred_confidences, pred_bboxes, bboxes = estimator.run(dataset_init,\n",
    "                                                          top_k=top_k[-1], n_max_test=n_max_tests)\n",
    "    iou_k = calc_iou_k(pred_bboxes, bboxes)\n",
    "    accuracy = ftfy_retrieval_accuracy(iou_k, top_k, iou_thrs)\n",
    "    all_accuracy.append(accuracy)\n",
    "\n",
    "    pred_bboxes = pred_bboxes[:, 0]\n",
    "    d_bboxes = np.abs(pred_bboxes - bboxes)\n",
    "\n",
    "    src_h, src_w = param.src_size\n",
    "    d_bboxes[..., 0] *= src_w\n",
    "    d_bboxes[..., 1] *= src_h\n",
    "    d_bboxes[..., 2] *= src_w\n",
    "    d_bboxes[..., 3] *= src_h\n",
    "    d_cx_mean, d_cx_std = np.mean(d_bboxes[..., 0]), np.std(d_bboxes[..., 0])\n",
    "    d_cy_mean, d_cy_std = np.mean(d_bboxes[..., 1]), np.std(d_bboxes[..., 1])\n",
    "    d_w_mean, d_w_std = np.mean(d_bboxes[..., 2]), np.std(d_bboxes[..., 2])\n",
    "    d_h_mean, d_h_std = np.mean(d_bboxes[..., 3]), np.std(d_bboxes[..., 3])\n",
    "\n",
    "    all_d_cx_mean.append(d_cx_mean)\n",
    "    all_d_cx_std.append(d_cx_std)\n",
    "    all_d_cy_mean.append(d_cy_mean)\n",
    "    all_d_cy_std.append(d_cy_std)\n",
    "    all_d_w_mean.append(d_w_mean)\n",
    "    all_d_w_std.append(d_w_std)\n",
    "    all_d_h_mean.append(d_h_mean)\n",
    "    all_d_h_std.append(d_h_std)\n",
    "\n",
    "    tf.logging.info('Avg. Retrieval Accuracy: {}'.format(accuracy))\n",
    "    tf.logging.info('For the best (@k=1), [mean, std]')\n",
    "    tf.logging.info('d_cx: {:.3f}, {:.3f}'.format(d_cx_mean, d_cx_std))\n",
    "    tf.logging.info('d_cy: {:.3f}, {:.3f}'.format(d_cy_mean, d_cy_std))\n",
    "    tf.logging.info('d_w : {:.3f}, {:.3f}'.format(d_w_mean, d_w_std))\n",
    "    tf.logging.info('d_h : {:.3f}, {:.3f}'.format(d_h_mean, d_h_std))\n",
    "    tf.logging.info('-' * 50)\n",
    "\n",
    "    # save checkpoint\n",
    "    if epoch % param.save_every == 0 or epoch+1 == param.n_epoch:\n",
    "        estimator.save(param.ftfy_scope, global_step=epoch)\n",
    "\n",
    "    if epoch == 5:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracy = np.asarray(all_accuracy)\n",
    "all_accuracy = np.squeeze(all_accuracy)\n",
    "plt.plot(all_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_d_cx_mean = np.squeeze(np.asarray(all_d_cx_mean))\n",
    "all_d_cy_mean = np.squeeze(np.asarray(all_d_cy_mean))\n",
    "all_d_w_mean = np.squeeze(np.asarray(all_d_w_mean))\n",
    "all_d_h_mean = np.squeeze(np.asarray(all_d_h_mean))\n",
    "\n",
    "all_d_cx_std = np.squeeze(np.asarray(all_d_cx_std))\n",
    "all_d_cy_std = np.squeeze(np.asarray(all_d_cy_std))\n",
    "all_d_w_std = np.squeeze(np.asarray(all_d_w_std))\n",
    "all_d_h_std = np.squeeze(np.asarray(all_d_h_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "ax = ax.ravel()\n",
    "\n",
    "N = len(all_d_cx_mean)\n",
    "ax[0].errorbar(range(N), all_d_cx_mean, all_d_cx_std, linestyle='None', marker='^')\n",
    "ax[1].errorbar(range(N), all_d_cy_mean, all_d_cy_std, linestyle='None', marker='^')\n",
    "ax[2].errorbar(range(N), all_d_w_mean, all_d_w_std, linestyle='None', marker='^')\n",
    "ax[3].errorbar(range(N), all_d_h_mean, all_d_h_std, linestyle='None', marker='^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "out_dir = os.path.join(param.log_dir, 'metrics_{}_{}.npy'.format(\n",
    "    param.train_datasets, param.train_datasets\n",
    "))\n",
    "metric = dict(\n",
    "    loss=np.array(all_loss),\n",
    "    accuracy=all_accuracy,\n",
    "    d_cx_mean=all_d_cx_mean,\n",
    "    d_cx_std=all_d_cx_std,\n",
    "    d_cy_mean=all_d_cy_mean,\n",
    "    d_cy_std=all_d_cy_std,\n",
    "    d_w_mean=all_d_w_mean,\n",
    "    d_w_std=all_d_w_std,\n",
    "    d_h_mean=all_d_h_mean,\n",
    "    d_h_std=all_d_h_std\n",
    ")\n",
    "np.save(out_dir, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
