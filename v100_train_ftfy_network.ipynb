{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from utils.Param import get_default_param\n",
    "from network.dataset.ftfy_patchdata import input_fn\n",
    "#from network.model_fn import triplet_model_fn\n",
    "#from network.dataset.sem_patchdata import input_fn\n",
    "#from network.dataset.sem_patchdata_ext import input_fn as sem_input_fn\n",
    "#from network.train import TripletEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproduction\n",
    "np.random.seed(2019)\n",
    "tf.set_random_seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters (adjust as needed)\n",
    "log_dir = './log/sem_ftfy'\n",
    "param = get_default_param(mode='AUSTIN', log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param.data_dir = '/home/sungsooha/Desktop/Data/ftfy/sem/train'\n",
    "#param.data_dir = './Data/austin'\n",
    "param.train_datasets = 'sem' # we will define sem dataset separately\n",
    "param.test_datasets = None #'human_patch'\n",
    "param.batch_size = 8 # 64 for v100\n",
    "param.n_epoch = 100\n",
    "param.n_triplet_samples = 500000\n",
    "param.train_log_every   = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['set_001', 'set_003', 'set_004', 'set_005', 'set_007', 'set_008', 'set_009', 'set_010', 'set_011', 'set_012', 'set_017', 'set_018', 'set_020', 'set_027', 'set_033', 'set_034', 'set_037', 'set_038', 'set_043', 'set_045', 'set_058', 'set_059', 'set_061', 'set_065']\n"
     ]
    }
   ],
   "source": [
    "sem_data_dir = '/home/sungsooha/Desktop/Data/ftfy/sem/train'\n",
    "sem_train_datasets = []\n",
    "for f in os.listdir(sem_data_dir):\n",
    "    if os.path.isdir(os.path.join(sem_data_dir,f)):\n",
    "        sem_train_datasets.append(f)\n",
    "sem_train_datasets = sorted(sem_train_datasets)\n",
    "print(sem_train_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Preparing data pipeline ...\n"
     ]
    }
   ],
   "source": [
    "tf.logging.info(\"Preparing data pipeline ...\")\n",
    "with tf.device('/cpu:0'), tf.name_scope('input'):\n",
    "    train_dataset, train_data_sampler = input_fn(\n",
    "        base_dir=sem_data_dir,\n",
    "        cellsz=param.cellsz,\n",
    "        n_parameters=param.n_parameters,\n",
    "        src_size=param.src_size,\n",
    "        tar_size=param.tar_size,\n",
    "        n_channels=1,\n",
    "        batch_size=param.batch_size\n",
    "    )\n",
    "    data_iterator = tf.data.Iterator.from_structure(\n",
    "        train_dataset.output_types,\n",
    "        train_dataset.output_shapes\n",
    "    )\n",
    "    train_dataset_init = data_iterator.make_initializer(train_dataset)\n",
    "    batch_data = data_iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset set_001/patches: 100%|██████████| 48/48 [00:00<00:00, 98.95it/s]\n",
      "Loading dataset set_001/sources: 100%|██████████| 5/5 [00:00<00:00, 17.19it/s]\n",
      "Loading dataset set_003/patches: 100%|██████████| 16/16 [00:00<00:00, 95.29it/s]\n",
      "Loading dataset set_003/sources: 100%|██████████| 2/2 [00:00<00:00, 17.85it/s]\n",
      "Loading dataset set_004/patches: 100%|██████████| 23/23 [00:00<00:00, 99.41it/s]\n",
      "Loading dataset set_004/sources: 100%|██████████| 3/3 [00:00<00:00, 18.04it/s]\n",
      "Loading dataset set_005/patches: 100%|██████████| 11/11 [00:00<00:00, 89.14it/s]\n",
      "Loading dataset set_005/sources: 100%|██████████| 2/2 [00:00<00:00, 18.62it/s]\n",
      "Loading dataset set_007/patches: 100%|██████████| 6/6 [00:00<00:00, 74.76it/s]\n",
      "Loading dataset set_007/sources: 100%|██████████| 1/1 [00:00<00:00, 15.06it/s]\n",
      "Loading dataset set_008/patches: 100%|██████████| 25/25 [00:00<00:00, 94.40it/s]\n",
      "Loading dataset set_008/sources: 100%|██████████| 3/3 [00:00<00:00, 15.31it/s]\n",
      "Loading dataset set_009/patches: 100%|██████████| 49/49 [00:00<00:00, 102.36it/s]\n",
      "Loading dataset set_009/sources: 100%|██████████| 5/5 [00:00<00:00, 14.73it/s]\n",
      "Loading dataset set_010/patches: 100%|██████████| 62/62 [00:00<00:00, 98.61it/s]\n",
      "Loading dataset set_010/sources: 100%|██████████| 7/7 [00:00<00:00, 17.42it/s]\n",
      "Loading dataset set_011/patches: 100%|██████████| 39/39 [00:00<00:00, 103.26it/s]\n",
      "Loading dataset set_011/sources: 100%|██████████| 4/4 [00:00<00:00, 16.66it/s]\n",
      "Loading dataset set_012/patches: 100%|██████████| 45/45 [00:00<00:00, 102.05it/s]\n",
      "Loading dataset set_012/sources: 100%|██████████| 5/5 [00:00<00:00, 17.39it/s]\n",
      "Loading dataset set_017/patches: 100%|██████████| 31/31 [00:00<00:00, 102.57it/s]\n",
      "Loading dataset set_017/sources: 100%|██████████| 4/4 [00:00<00:00, 18.48it/s]\n",
      "Loading dataset set_018/patches: 100%|██████████| 41/41 [00:00<00:00, 101.99it/s]\n",
      "Loading dataset set_018/sources: 100%|██████████| 5/5 [00:00<00:00, 17.85it/s]\n",
      "Loading dataset set_020/patches: 100%|██████████| 35/35 [00:00<00:00, 105.08it/s]\n",
      "Loading dataset set_020/sources: 100%|██████████| 4/4 [00:00<00:00, 17.09it/s]\n",
      "Loading dataset set_027/patches: 100%|██████████| 108/108 [00:01<00:00, 105.74it/s]\n",
      "Loading dataset set_027/sources: 100%|██████████| 11/11 [00:00<00:00, 17.47it/s]\n",
      "Loading dataset set_033/patches: 100%|██████████| 43/43 [00:00<00:00, 100.18it/s]\n",
      "Loading dataset set_033/sources: 100%|██████████| 5/5 [00:00<00:00, 16.81it/s]\n",
      "Loading dataset set_034/patches: 100%|██████████| 65/65 [00:00<00:00, 100.89it/s]\n",
      "Loading dataset set_034/sources: 100%|██████████| 7/7 [00:00<00:00, 17.33it/s]\n",
      "Loading dataset set_037/patches: 100%|██████████| 31/31 [00:00<00:00, 104.41it/s]\n",
      "Loading dataset set_037/sources: 100%|██████████| 4/4 [00:00<00:00, 15.69it/s]\n",
      "Loading dataset set_038/patches: 100%|██████████| 71/71 [00:00<00:00, 106.72it/s]\n",
      "Loading dataset set_038/sources: 100%|██████████| 8/8 [00:00<00:00, 17.98it/s]\n",
      "Loading dataset set_043/patches: 100%|██████████| 37/37 [00:00<00:00, 103.77it/s]\n",
      "Loading dataset set_043/sources: 100%|██████████| 4/4 [00:00<00:00, 17.54it/s]\n",
      "Loading dataset set_045/patches: 100%|██████████| 101/101 [00:00<00:00, 110.54it/s]\n",
      "Loading dataset set_045/sources: 100%|██████████| 11/11 [00:00<00:00, 18.04it/s]\n",
      "Loading dataset set_058/patches: 100%|██████████| 23/23 [00:00<00:00, 96.26it/s]\n",
      "Loading dataset set_058/sources: 100%|██████████| 3/3 [00:00<00:00, 18.40it/s]\n",
      "Loading dataset set_059/patches: 100%|██████████| 67/67 [00:00<00:00, 103.81it/s]\n",
      "Loading dataset set_059/sources: 100%|██████████| 7/7 [00:00<00:00, 15.79it/s]\n",
      "Loading dataset set_061/patches: 100%|██████████| 17/17 [00:00<00:00, 94.72it/s]\n",
      "Loading dataset set_061/sources: 100%|██████████| 2/2 [00:00<00:00, 16.55it/s]\n",
      "Loading dataset set_065/patches: 100%|██████████| 78/78 [00:00<00:00, 103.48it/s]\n",
      "Loading dataset set_065/sources: 100%|██████████| 8/8 [00:00<00:00, 16.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:datamap : (106010, 3)\n",
      "INFO:tensorflow:bboxes  : (106010, 4)\n",
      "INFO:tensorflow:targets : (106010, 128, 128, 1)\n",
      "INFO:tensorflow:sources : (10601, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data_sampler.load_dataset(\n",
    "    data_dirs=sem_train_datasets,\n",
    "    src_dir='sources', tar_dir='patches',\n",
    "    src_ext='bmp', src_size=param.src_size, n_src_channels=param.n_channels, \n",
    "    src_per_col=10, src_per_row=10,\n",
    "    tar_ext='bmp', tar_size=param.tar_size, n_tar_channels=param.n_channels,\n",
    "    tar_per_col=10, tar_per_row=10,\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading training stats: sem\n",
      "INFO:tensorflow:Calculating train data stats (mean, std)\n",
      "INFO:tensorflow:Mean: 0.30739\n",
      "INFO:tensorflow:Std : 0.21744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing targets: 100%|██████████| 106010/106010 [00:02<00:00, 37152.83it/s]\n",
      "Normalizing sources: 100%|██████████| 10601/10601 [00:01<00:00, 7418.14it/s]\n"
     ]
    }
   ],
   "source": [
    "tf.logging.info('Loading training stats: %s' % param.train_datasets)\n",
    "try:\n",
    "    file = open(os.path.join(param.log_dir, 'stats_%s.pkl' % param.train_datasets), \n",
    "                'rb')\n",
    "    mean, std = pickle.load(file)\n",
    "except:\n",
    "    tf.logging.info('Calculating train data stats (mean, std)')\n",
    "    mean, std = train_data_sampler.generate_stats()\n",
    "    pickle.dump(\n",
    "        [mean, std], \n",
    "        open(os.path.join(param.log_dir, 'stats_%s.pkl' % param.train_datasets), \n",
    "             'wb')\n",
    "    )\n",
    "tf.logging.info('Mean: {:.5f}'.format(mean))\n",
    "tf.logging.info('Std : {:.5f}'.format(std))\n",
    "train_data_sampler.normalize_data(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.info(\"Creating the model ...\")\n",
    "sources, targets, labels, bboxes = batch_data\n",
    "spec = ftfy_model_fn(\n",
    "    sources, targets, labels, bboxes, \n",
    "    # for triplet feature extractor\n",
    "    feature_mode='TEST',\n",
    "    feature_trainable=False,\n",
    "    feature_name='triplet-net'\n",
    "    cnn_name=param.cnn_name,\n",
    "    shared_batch_layers=True,\n",
    "    # for ftfy\n",
    "    ftfy_mode='TRAIN',  \n",
    "    optimizer_name=param.optimizer_name,\n",
    "    learning_rate=param.learning_rate,\n",
    ")\n",
    "#estimator = TripletEstimator(spec, save_dir=param.log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=[1, 5, 10, 20, 30]\n",
    "\n",
    "all_loss = [] # avg. loss over epochs\n",
    "train_fpr95 = [] # fpr95 with training dataset\n",
    "train_retrieval = [] # retrieval with training dataset\n",
    "test_fpr95 = []\n",
    "test_retrieval = []\n",
    "test_fpr95_2 = []\n",
    "test_retrieval_2 = []\n",
    "\n",
    "tf.logging.info('='*50)\n",
    "tf.logging.info('Start training ...')\n",
    "tf.logging.info('='*50)\n",
    "for epoch in range(param.n_epoch):\n",
    "    tf.logging.info('-'*50)\n",
    "    tf.logging.info('TRAIN {:d}, {:s} start ...'.format(epoch, param.train_datasets))\n",
    "    train_data_sampler.set_mode(0)\n",
    "    #train_data_sampler.set_n_triplet_samples(param.n_triplet_samples)\n",
    "    train_data_sampler.set_n_triplet_samples(5000)\n",
    "    loss = estimator.train(\n",
    "        dataset_initializer=train_dataset_init,\n",
    "        log_every=param.train_log_every\n",
    "    )\n",
    "    all_loss.append(loss)\n",
    "    tf.logging.info('-'*50)\n",
    "\n",
    "    # for evaluation with training dataset\n",
    "    tf.logging.info('-'*50)\n",
    "    tf.logging.info('TEST {:d}, {:s} start ...'.format(epoch, param.train_datasets))\n",
    "    train_data_sampler.set_mode(1)\n",
    "    train_data_sampler.set_n_matched_pairs(5000)\n",
    "    test_match = estimator.run_match(train_dataset_init)\n",
    "    fpr95 = fpr(test_match.labels, test_match.scores, recall_rate=0.95)\n",
    "    train_fpr95.append(fpr95)\n",
    "    tf.logging.info('FPR95: {:.5f}'.format(fpr95))\n",
    "    \n",
    "    train_data_sampler.set_mode(2)\n",
    "    test_rrr = estimator.run_retrieval(train_dataset_init)\n",
    "    rrr = retrieval_recall_K(\n",
    "        features=test_rrr.features,\n",
    "        labels=train_data_sampler.get_labels(test_rrr.index),\n",
    "        is_query=test_rrr.scores,\n",
    "        K=K\n",
    "    )[0]\n",
    "    train_retrieval.append(rrr)\n",
    "    tf.logging.info('Retrieval: {}'.format(rrr))\n",
    "    tf.logging.info('-'*50)\n",
    "    \n",
    "    break\n",
    "    \n",
    "    # for evaluation with test dataset\n",
    "    if param.test_datasets is not None:\n",
    "        tf.logging.info('-'*50)\n",
    "        tf.logging.info('TEST {:d}, {:s} start ...'.format(epoch, param.test_datasets))\n",
    "        test_data_sampler.set_mode(1)\n",
    "        #test_data_sampler.set_n_matched_pairs(1000)\n",
    "        test_match = estimator.run_match(test_dataset_init)\n",
    "        fpr95 = fpr(test_match.labels, test_match.scores, recall_rate=0.95)\n",
    "        test_fpr95.append(fpr95)\n",
    "        tf.logging.info('FPR95: {:.5f}'.format(fpr95))\n",
    "\n",
    "        test_data_sampler.set_mode(2)\n",
    "        test_rrr = estimator.run_retrieval(test_dataset_init)\n",
    "        rrr = retrieval_recall_K(\n",
    "            features=test_rrr.features,\n",
    "            labels=test_data_sampler.get_labels(test_rrr.index),\n",
    "            is_query=test_rrr.scores,\n",
    "            K=K\n",
    "        )[0]\n",
    "        test_retrieval.append(rrr)\n",
    "        tf.logging.info('Retrieval: {}'.format(rrr))\n",
    "        tf.logging.info('-'*50)\n",
    "    \n",
    "    # for evaluation with test dataset\n",
    "    if test_datasets is not None:\n",
    "        tf.logging.info('-'*50)\n",
    "        tf.logging.info('TEST {:d}, {:s} start ...'.format(epoch, test_datasets))\n",
    "        test_data_sampler_2.set_mode(1)\n",
    "        #test_data_sampler.set_n_matched_pairs(1000)\n",
    "        test_match = estimator.run_match(test_dataset_init_2)\n",
    "        fpr95 = fpr(test_match.labels, test_match.scores, recall_rate=0.95)\n",
    "        test_fpr95_2.append(fpr95)\n",
    "        tf.logging.info('FPR95: {:.5f}'.format(fpr95))\n",
    "\n",
    "        test_data_sampler_2.set_mode(2)\n",
    "        test_rrr = estimator.run_retrieval(test_dataset_init_2)\n",
    "        rrr = retrieval_recall_K(\n",
    "            features=test_rrr.features,\n",
    "            labels=test_data_sampler_2.get_labels(test_rrr.index),\n",
    "            is_query=test_rrr.scores,\n",
    "            K=K\n",
    "        )[0]\n",
    "        test_retrieval_2.append(rrr)\n",
    "        tf.logging.info('Retrieval: {}'.format(rrr))\n",
    "        tf.logging.info('-'*50)\n",
    "    \n",
    "    # save checkpoint\n",
    "    if epoch % param.save_every == 0 or epoch+1 == param.n_epoch:\n",
    "        estimator.save(param.project_name, global_step=epoch)\n",
    "    \n",
    "    #if epoch > 10:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].plot(train_fpr95)\n",
    "ax[1].plot(test_fpr95)\n",
    "ax[2].plot(test_fpr95_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].plot(train_retrieval)\n",
    "ax[1].plot(test_retrieval)\n",
    "ax[2].plot(test_retrieval_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "out_dir = os.path.join(param.log_dir, 'metrics_{}_{}.npy'.format(\n",
    "    param.train_datasets, param.train_datasets\n",
    "))\n",
    "metric = dict(\n",
    "    loss=np.array(all_loss),\n",
    "    fpr95=np.array(train_fpr95),\n",
    "    retrieval=np.asarray(train_retrieval)\n",
    ")\n",
    "np.save(out_dir, metric)\n",
    "\n",
    "out_dir = os.path.join(param.log_dir, 'metrics_{}_{}.npy'.format(\n",
    "    param.train_datasets, param.test_datasets\n",
    "))\n",
    "metric = dict(\n",
    "    loss=np.array(all_loss),\n",
    "    fpr95=np.array(test_fpr95),\n",
    "    retrieval=np.asarray(test_retrieval)\n",
    ")\n",
    "np.save(out_dir, metric)\n",
    "\n",
    "out_dir = os.path.join(param.log_dir, 'metrics_{}_{}.npy'.format(\n",
    "    param.train_datasets, test_datasets\n",
    "))\n",
    "metric = dict(\n",
    "    loss=np.array(all_loss),\n",
    "    fpr95=np.array(test_fpr95_2),\n",
    "    retrieval=np.asarray(test_retrieval_2)\n",
    ")\n",
    "np.save(out_dir, metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
